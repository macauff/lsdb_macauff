{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing macauff results into HiPSCat / LSDB\n",
    "\n",
    "The macauff pipeline OUTSIDE of LSDB will create a series of yml and csv files\n",
    "that represent the counterpart assignments and likelihoods between objects in \n",
    "two catalogs.\n",
    "\n",
    "To convert into an LSDB-friendly association table, you will need:\n",
    "- match file schema file (either yaml or parquet schema)\n",
    "- match output CSVs\n",
    "- pre-hipscatted left catalog\n",
    "- pre-hipscatted right catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema\n",
    "\n",
    "If you have a yaml schema, you will first need to convert it into a parquet\n",
    "schema to be understood by the import pipeline.\n",
    "\n",
    "Use the `from_yaml` utility method to read the yaml files; parse column names,\n",
    "types, and other key-values; convert into parquet types; write to an empty\n",
    "parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hipscat.io import file_io\n",
    "from lsdb_macauff.import_pipeline.convert_metadata import from_yaml\n",
    "from pathlib import Path\n",
    "\n",
    "## Location of the data on UW internal servers\n",
    "epyc_input_path = Path(\"/data3/epyc/data3/hipscat/raw/macauff_results/\")\n",
    "\n",
    "yaml_input_file = epyc_input_path / \"metadata\" / \"macauff_metadata.yml\"\n",
    "from_yaml(yaml_input_file, epyc_input_path / \"metadata\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the parquet/pyarrow schema that's generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_schema_file = epyc_input_path / \"metadata\" / \"macauff_GaiaDR3xCatWISE2020_matches.parquet\"\n",
    "single_metadata = file_io.read_parquet_metadata(matches_schema_file)\n",
    "schema = single_metadata.schema.to_arrow_schema()\n",
    "schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association table\n",
    "\n",
    "To create the association between two catalogs, use the purpose-built Macauff\n",
    "association table import map-reduce pipeline.\n",
    "\n",
    "See also the general documentation for pipeline argument configuration \n",
    "for the [hipscat-import pipeline](https://hipscat-import.readthedocs.io/en/latest/catalogs/arguments.html#pipeline-setup). \n",
    "The macauff pipeline uses that pipeline construction directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lsdb_macauff.import_pipeline.run_import as runner\n",
    "from hipscat_import.catalog.file_readers import CsvReader\n",
    "from lsdb_macauff.import_pipeline.arguments import MacauffArguments\n",
    "from dask.distributed import Client\n",
    "import glob\n",
    "\n",
    "matches_schema_file = epyc_input_path / \"metadata\" / \"macauff_GaiaDR3xCatWISE2020_matches.parquet\"\n",
    "## Find all of the CSV files under the macauff output directory.\n",
    "macauff_data_dir = epyc_input_path / \"rds/project/iris_vol3/rds-iris-ip005/tjw/dr3_catwise_allskytest/output_csvs/\"\n",
    "files = glob.glob(f\"{macauff_data_dir}/**/*.csv\")\n",
    "files.sort()\n",
    "\n",
    "args = MacauffArguments(\n",
    "    ## This will create an association catalog at the path:\n",
    "    ##    /data3/epyc/data3/hipscat/catalogs/macauff_association/\n",
    "    output_path=\"/data3/epyc/data3/hipscat/catalogs/\",\n",
    "    output_artifact_name=\"macauff_association\",\n",
    "\n",
    "    ## Make sure you use a directory with enough space!\n",
    "    tmp_dir=\"/data3/epyc/data3/hipscat/tmp/macauff/\",\n",
    "\n",
    "    ## Read the CSV files and use the generated schema file for types and \n",
    "    ## other key-value metadata\n",
    "    input_file_list=files,\n",
    "    input_format=\"csv\",\n",
    "    file_reader=CsvReader(schema_file=matches_schema_file, header=None),\n",
    "    metadata_file_path=matches_schema_file,\n",
    "\n",
    "    ## For left catalog, specify the pre-hipscatted location, and ra/dec columns\n",
    "    left_catalog_dir=\"/data3/epyc/data3/hipscat/catalogs/gaia_dr3/gaia\",\n",
    "    left_ra_column=\"gaia_ra\",\n",
    "    left_dec_column=\"gaia_dec\",\n",
    "    left_id_column=\"gaia_source_id\",\n",
    "\n",
    "    ## For right catalog, specify the pre-hipscatted location, and ra/dec columns\n",
    "    right_catalog_dir=\"/epyc/projects3/sam_hipscat/catwise2020/catwise2020/\",\n",
    "    right_ra_column=\"catwise_ra\",\n",
    "    right_dec_column=\"catwise_dec\",\n",
    "    right_id_column=\"catwise_name\",\n",
    "\n",
    ")\n",
    "\n",
    "with Client(\n",
    "    local_directory=\"/data3/epyc/data3/hipscat/tmp/macauff/\",\n",
    "    n_workers=5,\n",
    "    threads_per_worker=1,\n",
    ") as client:\n",
    "    runner.run(args, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could take a while. Once it's done, check that the association data can be parsed as a valid association catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hipscat.catalog.association_catalog.association_catalog import AssociationCatalog\n",
    "\n",
    "catalog = AssociationCatalog.read_from_hipscat(args.catalog_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
